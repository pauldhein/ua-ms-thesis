\chapter{INTRODUCTION\label{chapter:introduction}}
When studying a given natural phenomena, many researchers turn to modeling as a method to formalize their reasoning about the phenomena. Scientific models are a way of studying how phenomena in the natural world affect one another. Using models to study phenomena also comes with many advantages for researchers, including:
\begin{itemize}
  \item Models allow researchers to clearly communicate the relations between variables observed to be associated with a phenomena.
  \item Models explicitly represent the uncertainty present when studying a phenomena, and allow the uncertainty to be combined with what is known about the phenomena
  \item Models are exportable, comparable, and updatable. One researcher can use the model of another, competing models of the same phenomena can be compared, and under-performing components of a model can be updated upon discovering new information about the phenomena under study.
\end{itemize}

These advantages have been the driving force that has pushed the scientific community towards models as a method of communication of scientific research. To get a better of sense of what these models are like we can examine the model below that is being used to study how the yield of a particular crop is affected by changes in the amount of rain and absorption of the soil over a range of days.

\begin{figure}[ht]
  \begin{center}
    \begin{tabular}{cc}
      \tikz{ % Simple Crop Yield model example
        \tikzstyle{readable}=[rectangle, thick, rounded corners]
        \node[latent, readable] (crop_yield) {$Yield$} ; %
        \node[latent, readable, above=of crop_yield] (total_rain) {$Rain_{total}$} ; %
        \node[latent, readable, above=of total_rain] (rain) {$Rain$} ; %
        \node[obs, readable, above=of rain] (max_rain) {$Rain_{max}$} ; %
        \node[obs, readable, left=of max_rain] (absorption) {$Absorption$} ; %
        \node[obs, readable, right=of max_rain] (consistency) {$Consistency$} ; %
        \node[obs, readable, right=of rain] (day) {$Day$} ; %
        \edge {day, consistency, absorption, max_rain} {rain} ; %
        \edge {rain} {total_rain} ; %
        \path [->] (total_rain) edge  [loop right] (total_rain);
        \edge {total_rain} {crop_yield} ; %

        \plate {loop} {(rain)(day)(total_rain)} {$Day$} ;
      }
    \end{tabular}
  \end{center}
  \caption[Crop yield model]{A model depicting the affects of rain on the yield of a crop over a span of days given observed values for absorption and consistency constants.}
\end{figure}

In this model we see a set of input values, shaded to show that they are observed, and an output value. Upon observing the inputs through measurement, a value for the output can be recovered. We can also see in the model above that there is an arbitrary amount of wiring that can occur to transform a set of model inputs into the model output. Along with this arbitrary wiring, it is also worth noting that, given an output phenomena to study, the set of chosen observable inputs is also arbitrary. From these two facts we arrive at one of the modern challenges associated with modeling, models have become ubiquitous. In many fields there are so many competing models for the same phenomena that modelers have entered an age marked by \textit{The Paradox of Choice}, where modelers must now spend a non-trivial amount of time deciding which model to use for their experiments. This task is formally known as the task of \textit{Model Selection}.

 Not only do modelers have to explore many competing models when deciding which to use to model a particular phenomena, this exploration is also expensive. In the information age, many of these models exist as source code with associated grounding documents. However, with the extreme prevalence of programming languages many competing models for the same phenomena are likely written in different programming languages. Asking modelers to learn a single new programming language is already a large drain on research time, but the prospect of needing to learn multiple new languages represents a barrier to entering the realm of model selection for most researchers. Given the enormous amount of items competing for the limited time of modelers the task of model selection is commonly side-lined. This entails that in many fields a single or small set of models are used and hardly ever challenged. Not only that but this means that many researchers who create competing models will see their research go unused by the members of the field that the new model was intended to benefit.

\section{Problem Scope\label{sec:prob_scope}}
Creating a tool that largely automates the task of model selection is the main goal of this thesis; however, the task of model selection is not the only problem that this thesis seeks to address. The larger problem addressed by this thesis is the problem of creating a framework that allows the task of model selection to be performed autonomously.

Therefore the scope of the problem addressed by this thesis has four components:
\begin{enumerate}
  \item The system must be able to extract models from scientific source code. This method must be generalizable to all programming languages, and it must produce models that are executable.
  \item The system must be able to compare competing models of the same phenomena to determine how they are similar and how they differ.
  \item The system must be able to analyze the extracted models to develop data that can be used to differentiate one model from the other.
  \item The system must have a defined set of outputs that can, at the very least, be presented to the user that allow the user to easily choose which model to use between competing models of the same phenomena.
\end{enumerate}

With all four of these components in place the system will have automated the tedious portions of model selection, and will allow modelers to avoid many of the hurdles outlined in the section above that prevent them from effectively conducting model selection. Such a system, given additional information about the qualities of a model that would make one model more desirable than another, should also then be able to select a best-fit model for an experiment. Therefore the final product from the system should be a selected model from a set of extracted models, accompanied with the information gained by the system during the analysis phase in order to make the selection. A modeler would then be able to either utilize the model selected by the system or use the systems analysis output to inform their own decision.


\section{Related Work\label{sec:related_work}}
The prevalence of scientific models has caused the task of model selection to gain large amounts of attention from the scientific community. As such there are existing systems that provide potential solutions or assistance in tackling the model selection problem that are worth mentioning as context for this thesis.

For modelers in many disciplines the current state-of-the art for addressing the problem of model selection is not to use an intelligent tool or service that performs the selection, but to dig into the literature and find examples of comparisons that have already been done amongst competing models for a given phenomena. An example of this can be found models in the DSSAT corpus that has been used as a reference model throughout this thesis. % TODO: add citation of reference analysis study done between DSSAT and other models
Unsurprisingly, this approach has many disadvantages. The obvious initial cost of this approach is that it requires a group of scientists to devote a substantial portion of time to conducting this analysis by hand. A second disadvantage of this method is that the analysis performed and documented by research groups in scientific publications are difficult to extend. For example the analysis conducted by Camagaro et al. contains information on the sensitivity of the compared models to certain inputs; however, modelers are left without any discussion or investigation into the pairwise affect of inputs on model sensitivity. In order for modelers to obtain this information there best option is to request access to the software used by the authors so that they can manually extend the software to extend the analysis. This is a less-than-ideal solution as not only will additional time be required to extend the analysis, but modelers must now rely upon the generosity of the authors to release the software they used to conduct their analysis, otherwise extending the analysis presented would require conducting all of the software design necessary to produce the initial analysis. One final disadvantage of this approach is that relying upon model analysis reports found in the literature places an artificial restriction upon the phenomena that modelers are likely to study and the models they are likely to use. This limitation entails that fields where less time and fewer research funds have been spent on the task of model analysis will have a marginalized ability to conduct model selection. Automating the process of model extraction, analysis, and selection therefore not only increases the extensibility of an analysis but also increases the diversity of scientific research and discovery by placing disparate fields on a more equal footing.

Current tools such as the DAKOTA system allow modelers access to analysis tools such as the sensitivity methods utilized in the analysis section of SMS pipeline. DAKOTA has been an excellent resource for modelers because of the extent of evaluation methods that are provided to modelers for model study. However DAKOTA has some disadvantages that are addressed by the SMS pipeline presented in this thesis. The most notable disadvantage of the DAKOTA system is that modelers are still required to undergo the labor-intensive class of translating their models by hand into the pre-defined DAKOTA format. Not only does the labor and time present a barrier to entry for modelers wishing to use the DAKOTA system, but this system comes with an additional barrier for modelers who wish to use models from other researchers. While the modelers will likely be familiar with the structure of their own models, allowing for a simple translation task, they will likely require additional time and mental labor to become well-acquainted with other groups model specifications in order to create a faithful translation of the groups model to the required DAKOTA format. Not only is this an additional cost and barrier to entry for modelers, but it also introduces an unnecessary potential source of error.
% TODO: add info about DAKOTA not including any mention of model comparison. such actions require more manual effort by the users

New methods have been created in recent years that are beginning to automate the process of extracted scientific models from free text. One such method is presented in the Eidos, INDRA, and Delphi software tool that seeks to extract models from textual documents, such as scientific publications and technical reports. This software has seen great success at the task of model extraction and assembly, both of which are tasks conducted by the SMS pipeline. The approach presented in this thesis differs from the approach of Eidos, INDRA, and Delphi in that while the later focuses on model extraction from free text, the focus of this work is on leveraging the definitive structure found in source code to create models with high confidence from the large collection of existing source code repositories.


\section{This Work\label{sec:this_work}}
In this thesis I will present the Software-derived Model Selection (SMS) pipeline; an automated system that seeks the select the appropriate scientific model for a given experiment from a selection of models that have been extracted from source code. To accomplish this task the system must be able to extract scientific models from source code, ground the real-world variables contained in the models using information gained from associated texts, and finally perform the model selection task using sensitivity analysis. Accomplishing this task will further unlock the potential of models to revolutionize the objective study of naturally occurring phenomena.

The remainder of this thesis is organized to present the components of the pipeline that will solve the problem presented in the problem scope section. Chapter II will introduce the algorithms used to extract models from source code and transform them into a form that is both executable and comparable across competing models. Chapter III will introduce the analysis methods used by the SMS pipeline to derive information about the models under comparison. Chapter IV will document how the derived information from the analysis phase can be used to perform automated model selection, as well as how the information will be presented to users of the SMS pipeline to allow them to make their own final model selection decisions. This thesis will then conclude with Chapter V that will be a discussion of the results and implications of the pipeline and will introduce possible extensions for continuing this research.
