\chapter{INTRODUCTION\label{chapter:introduction}}
\ctm{Scientific models are increasingly expressed as executable software.
This enables a host of opportunities, including the following:
\begin{itemize}
\item increased precision in predictions,
\item the possibility of better control over reproducibility of results,
\item better communication of model details through unambiguous code implementation, and
\item the potential to aggregate individual models from different domains into larger multi-domain models.
\end{itemize}}

% CTM original:
%When studying a given natural phenomena, many researchers turn to modeling as a method to formalize their reasoning about the phenomena. Scientific models are a way of studying how phenomena in the natural world affect one another. Using models to study phenomena also comes with many advantages for researchers, including:
%\begin{itemize}
%  \item Models allow researchers to clearly communicate the relations between variables observed to be associated with a phenomena.
%  \item Models explicitly represent the uncertainty present when studying a phenomena, and allow the uncertainty to be combined with what is known about the phenomena
%  \item Models are exportable, comparable, and updatable. One researcher can use the model of another, competing models of the same phenomena can be compared, and under-performing components of a model can be updated upon discovering new information about the phenomena under study.
%\end{itemize}

These advantages have been the driving force that has pushed the scientific community towards \ctm{computational} models as a method of \ctm{communicating} scientific research.
\ctm{To provide a working example, consider the computational model below in Figure~\ref{fig:simple_crop_CAG}, expressed as a \emph{causal analysis graph} (CAG), where nodes represent variables and directed arcs represent a functional relationship between variables (this formalism will be described in more detail in Chapter~\ref{chapter:extraction}). This model describes how the yield of a particular crop is affected by changes in the amount of rain and soil water absorption rate over time (expressed in \emph{Day}s).}

% CTM original:
% To get a better of sense of what these models are like we can examine the model below that is being used to study how the yield of a particular crop is affected by changes in the amount of rain and absorption of the soil over a range of days.

\begin{figure}[!htbp]
  \label{fig:simple_crop_CAG}
  \centering
  \tikz{ % Simple Crop Yield model example
    \tikzstyle{readable}=[rectangle, thick, rounded corners]
    \node[latent, readable] (crop_yield) {$Yield$} ; %
    \node[latent, readable, above=of crop_yield] (total_rain) {$Rain_{total}$} ; %
    \node[latent, readable, above=of total_rain] (rain) {$Rain$} ; %
    \node[obs, readable, above=of rain] (max_rain) {$Rain_{max}$} ; %
    \node[obs, readable, left=of max_rain] (absorption) {$Absorption$} ; %
    \node[obs, readable, right=of max_rain] (consistency) {$Consistency$} ; %
    \node[obs, readable, right=of rain] (day) {$Day$} ; %
    \edge {day, consistency, absorption, max_rain} {rain} ; %
    \edge {rain} {total_rain} ; %
    \path [->] (total_rain) edge  [loop right] (total_rain);
    \edge {total_rain} {crop_yield} ; %

    \plate {loop} {(rain)(day)(total_rain)} {$Day$} ;
  }
  \tikz{ % Different Crop Yield model example
    \tikzstyle{readable}=[rectangle, thick, rounded corners]
    \node[latent, readable] (crop_yield) {$Yield$} ; %
    \node[latent, readable, above=of crop_yield] (total_rain) {$Rain_{total}$} ; %
    \node[latent, readable, above=of total_rain] (rain) {$Rain$} ; %
    \node[obs, readable, above=of rain] (max_rain) {$Rain_{max}$} ; %
    \node[obs, readable, right=of max_rain] (absorption) {$Absorption$} ; %
    \node[obs, readable, left=of rain] (consistency) {$Consistency$} ; %
    \node[obs, readable, left=of total_rain] (sunlight) {$Sunlight$} ; %
    \node[obs, readable, right=of rain] (day) {$Day$} ; %
    \edge {day, absorption, max_rain} {rain} ; %
    \edge {rain, consistency} {total_rain} ; %
    \path [->] (total_rain) edge  [loop right] (total_rain);
    \edge {total_rain, sunlight} {crop_yield} ; %

    \plate {loop} {(rain)(day)(total_rain)} {$Day$} ;
  }
  \caption[Competing models of crop yield]{Two competing scientific models depicting the affects of rain on the yield of a crop over a span of days given some other inputs. We see that the two models share many of their inputs but that some inputs may not be shared and the wiring of the inputs to the output variable can differ between the models.}

\end{figure}

In this model we see a set of input values, shaded to \ctm{indicate} % show
that they are observed, and an output value. Upon observing the inputs through measurement, a value for the output can be \ctm{computed.} % recovered.

\ctm{[PAUL: I recommend that you rework the next two paragraphs (which I've now commented-out) -- I think the message gets a little lost -- e.g., it's not clear from this single figure that there could be ``arbitrary wiring''. Here's what I suggest: Add a second figure to Fig 1.1 with the same output, perhaps with two inputs shared, but a third that is different than the third in the original model, and with different internal ``wiring''.
Here are the points I think you want to make:
(1) Say this up front: The end goal is to provide tools by which human modelers can more effectively do comparison/contrast between models, to help with selecting (and possibly adapting, although that's for future work) existing models appropriate for a modeling application. This is helping with model selection, but be careful to clarify that although this is related, it is not the same thing as \emph{model selection} in machine learning (which involves automating a search over models within a model family by systematically varying model parameters to better fit data).
(2) The two example variants (once you add the second example) demonstrate that although they compute the same output, there is a relationship between the models but they're not identical even though they compute the same output: (a) the inputs can be different (they could even not overlap at all); (b) the internal wiring between inputs and outputs can be different.
(3) These models might be implemented in different programming languages; hence, we need a method to extract the ``computational graph'' that relates how variables are related by functions in a programming-language-agnostic representation (i.e., GrFN).
(4) Be up front that, of course, we can only then compare two models in this pl-agnostic representation under the assumption that we know (or have strong evidence that) the variables that do match are referring to the same aspects of the modeled world (the same world domain); grounding itself is not part of your thesis work, but you are making use of the other parts of the AutoMATES system that perform variable grounding; you then use grounded variables in the central representation (GrFN) to do model analysis (comparison and sensitivity analysis).
]}

% CTM Original:
%We can also see that there is an arbitrary amount of wiring that can occur to transform a set of model inputs into the model output. Along with this arbitrary wiring, it is also worth noting that, given an output phenomena to study, the set of chosen observable inputs is also arbitrary. From these two facts we arrive at one of the modern challenges associated with modeling, models have become ubiquitous. In many fields there are so many competing models for the same phenomena that modelers have entered an age marked by \textit{The Paradox of Choice}, where modelers must now spend a non-trivial amount of time deciding which model to use for their experiments. This task is formally known as the task of \textit{Model Selection}.

% CTM Original:
%Not only do modelers have to explore many competing models when deciding which to use to model a particular phenomena, this exploration is also expensive. In the information age, many of these models exist as source code with associated grounding documents. However, with the extreme prevalence of programming languages many competing models for the same phenomena are likely written in different programming languages. Asking modelers to learn a single new programming language is already a large drain on research time, but the prospect of needing to learn multiple new languages represents a barrier to entering the realm of model selection for most researchers. Given the enormous amount of items competing for the limited time of modelers the task of model selection is commonly side-lined. This entails that in many fields a single or small set of models are used and hardly ever challenged. Not only that but this means that many researchers who create competing models will see their research go unused by the members of the field that the new model was intended to benefit.

\section{Problem Scope\label{sec:prob_scope}}

\ctm{This thesis describes a set of related data structures and algorithms that are part of a framework aimed at automatubg parts of the task of analyzing scientific models implemented in software. In particular, this thesis addresses the following four tasks underlying automated computational model analysis and selection:}

% CTM Original:
% Creating a tool that largely automates the task of model selection is the main goal of this thesis; however, the task of model selection is not the only problem that this thesis seeks to address. The larger problem addressed by this thesis is the problem of creating a framework that allows the task of model selection to be performed autonomously.

% CTM Original:
%Therefore the scope of the problem addressed by this thesis has four components:
\begin{enumerate}
  \item % The system must be able to extract
  \ctm{A processing pipeline to extract} models from scientific source code \ctm{and represent them in a source programming language-agnostic representation that supports general model analysis}.
%  . This method must be generalizable to all programming languages, and it must produce models that are executable.
  \item % The system must be able to
  \ctm{An algorithm to identify model structural overlap, as a basis for identifying how models are structurally similar and different.}
  % Comparing competing models of the same phenomena to determine how they are similar and how they differ.
  \item  % The system must be able to analyze the extracted models to develop data that can be used to differentiate one model from the other.
  \ctm{An algorithm for searching for input value ranges that make overlapping models behave as close to each other as possible, and thereby also identify value ranges that distinguish models (a basis for experiment design).}
  \item  % The system must have a defined set of outputs that can, at the very least, be presented to the user that allow the user to easily choose which model to use between competing models of the same phenomena.
  \ctm{Adaptation of sensitivity analysis measures to identify model output sensitivity as a function of model input value ranges within a uniform analysis framework.}
\end{enumerate}

\ctm{Combining these capabilities into a single framework provides a facility for domain-expert model developers and analysts to now analyze and compare models within a uniform framework, greatly simplifying model analysis tasks that to-date have required enormous manual effort.
This system does not take the human out of the loop: domain expertise and human guidance are still needed to identify variable value ranges of interest to a modeling application, as well as supplement mistakes of omission and commission that may be made during variable grounding.
Ongoing and future development is also required to scale the methods to effectively handle larger model code bases.
However, this framework described here is, to our knowledge, the first general approach to automating aspects of model analysis in the support of general model comparison and selection.
We also believe these tools provide a basis for a new kind of model curation and debugging, allowing one to compare changes within evolving code bases but from a modeling domain-semantics perspective. Exploring use of this framework for this purpose will be the subject of future work.
[PAUL: possibly move or reiterate parts of these ideas to the conclusion]}

% CTM Original:
%With all four of these components in place the system will have automated the tedious portions of model selection, and will allow modelers to avoid many of the hurdles outlined in the section above that prevent them from effectively conducting model selection. Such a system, given additional information about the qualities of a model that would make one model more desirable than another, should also then be able to select a best-fit model for an experiment. Therefore the final product from the system should be a selected model from a set of extracted models, accompanied with the information gained by the system during the analysis phase in order to make the selection. A modeler would then be able to either utilize the model selected by the system or use the systems analysis output to inform their own decision.


\section{\ctm{Related} Work\label{sec:prior_work}}  % CTM Original: Related
For modelers in many disciplines the current state-of-the art for addressing the problem of model selection is not to use an intelligent tool or service that performs the selection, but to dig into the literature and find examples of \ctm{model} comparisons that have already been done amongst competing models in a given application domain.
Unsurprisingly, this approach has many disadvantages.
The obvious initial cost of this approach is that it requires a group of scientists to devote a substantial portion of time to conducting this analysis by hand.
A second disadvantage of this method is that the analysis performed and documented by research groups in scientific publications is difficult to extend.
For example the analysis conducted by \citet{camargo2016six} contains information on the sensitivity of the compared models to certain inputs; however, modelers are left without any discussion or investigation into the pairwise affect of inputs on model sensitivity.
In order for modelers to obtain this information, their best option is to request access to the software used by the authors so that they can manually extend the software to extend the analysis.
This is a less-than-ideal solution as not only will additional time be required to extend the analysis, but modelers must now rely upon the generosity of the authors to release the software they used to conduct their analysis, otherwise extending the analysis presented would require conducting all of the software design necessary to produce the initial analysis. One final disadvantage of the current approach is that relying upon model analysis reports found in the literature places an artificial restriction upon the phenomena that modelers are likely to study and the models they are likely to use.
\ctm{[PAUL: Explain this last point; what is the restriction? Can you give an example?]}
This limitation entails that fields where less time and fewer research funds have been spent on the task of model analysis will have a marginalized ability to conduct model selection.
Automating the process of model extraction, analysis, and selection therefore not only increases the extensibility of an analysis but also increases the diversity of scientific research and discovery by placing disparate fields on a more equal footing.

Current tools such as the DAKOTA system make advances towards a generalized framework that provides modelers analysis tools that can be used to study and compare models \citep{adams2009dakota}.
DAKOTA has been an excellent resource for modelers because of the many evaluation methods that are provided for model study.
However DAKOTA has some disadvantages.
The most notable disadvantage of the DAKOTA system is that modelers are still required to undergo the labor-intensive task of translating their models by hand into the pre-defined DAKOTA format.
Not only does the labor and time present a barrier to entry for modelers wishing to use the DAKOTA system, but this system comes with an additional barrier for modelers who wish to use models from other researchers.
While the modelers will likely be familiar with the structure of their own models, allowing for a simple translation task, they will likely require additional time and mental labor to become well-acquainted with model specifications \ctm{developed for the models in other software systems}
in order to create a faithful translation of the groups model to the required DAKOTA format.
% CTM Original: Not only is this an additional cost and barrier to entry for modelers, but it also
\ctm{Adaptation or reimplementation of other code bases}
introduces an unnecessary potential source of error. Another shortcoming of the DAKOTA system is that it does not provide any service to \ctm{\emph{compare}} models. Modelers may be interested in what variables overlap between competing models, and may wish to augment the existing models to allow for analysis upon the overlapped portions of two competing models.
DAKOTA does not offer any \ctm{approach to help with this task.} % CTM Original: framework to handle this task.
This means that while DAKOTA is a very useful tool for modelers to analyze existing models that they are well-acquainted enough to translate, it does not
\ctm{provide help in performing larger scale model selection,}
% solve the problem of automating the task the model selection,
as modelers are still required to do large amounts of setup, reconstruction, and investigation of analysis methods in order to gather useful information from DAKOTA that they can then use to select a model.

New methods have been created in recent years that are beginning to automate the process of \ctm{extracting} % CTM Original: extracted
scientific models from free text. One such method is presented in the Eidos, INDRA, and Delphi \citep{EidosIndraDelphi} \ctm{pipeline} % software tool
that seeks to extract models from textual documents, such as scientific publications and technical reports. This software has seen great success at the task of model extraction and assembly. Indeed many models are presented in the scientific literature and sometimes only in the scientific literature without being present in the form of source code in a software repository. However modelers will undoubtedly benefit from the reliability guarantees of models that are extracted from source code.
% CTM Original: Firstly,
% CTM: PAUL: (1) don't use 'firstly' -- just say 'first'; (2) don't start a sentence with "first" unless you have a "second"! In this case, you're not really enumerating, you're making a contrast.
\ctm{Due} to the nature of free text, identifying and extracting models from free text is
\ctm{very challenging: authors are not forced to write effective procedures, instead describing models at higher levels of descriptions and often eliding information assumed as common-sense (or domain-specific) background knowledge.}
% CTM Original: inherently a probabilistic task. Thus models that are extracted from text are prone to include errors.
\ctm{Source} code \ctm{, on the other hand,} is explicit.
\ctm{no information can be left out if it is to execute:}
%, and thus, given the proper framework, it should be possible to extract models from source code that are entirely faithful to the initial representation of the model in the source code. % The explicit nature of s
\ctm{source code also requires full specification in order to be executable, so}
models extracted from source code \ctm{may include background assumptions that are left out of textual descriptions.} % will be fully specified, whereas it is possible for a model extracted from free text to be fully faithful to the model as described in the text and to still be underspecified.
A simple example of this phenomena would be the lack of data type information for numerical values. This information is commonly absent from the text descriptions of a model, but is vital in many cases when creating an executable version of the model.

However, extracting models from source code does share at least one of the challenges associated with extracting models from free text:
% CTM Original:. This challenge is the challenge of
\ctm{associating or \emph{grounding}} the variables present in the source code to the \ctm{aspects of the domain being modeled}.
natural phenomena they represent.
% CTM Original: Thus t
\ctm{The} approach presented in this thesis
% CTM Original: can be viewed as an extension of
\ctm{makes use of} the approach of the Eidos, INDRA, and Delphi system \ctm{to grounding textual mentions of concepts to domain taxonomies}.
% CTM Original: . In fact the system presented here utilizes the work of the Eidos, INDRA, and Delphi system in order to tackle the aforementioned task of variable grounding.

\section{This Work\label{sec:this_work}}
In this thesis I will present the Scientific Model Selection (SMS) pipeline \ctm{:}  % CTM Original: ;
an % CTM Original: automated
system that
\ctm{provides a set of tools to help with the comparison and selection of models extracted from source code}
% CTM Original: seeks the select the appropriate scientific model for a given experiment from a selection of models that have been extracted from source code.
To accomplish this task the system must be able to extract scientific models from source code, ground the real-world variables contained in the models using information gained from associated texts, and finally perform the model selection task using information gained from sensitivity analysis.
\ctm{PAUL: possibly move this to the conclusion??: Accomplishing this task will further unlock the potential of models to revolutionize the objective study of naturally occurring phenomena.}
The SMS pipeline is a component of the larger software system designed as part of the DARPA funded Automated Model Assembly from Text Equations and Software (AutoMATES\footnote{\url{https://ml4ai.github.io/automates/}}) project.

As a component of the AutoMATES project, this pipeline focuses on part of the overall goal of model extraction, grounding, and analysis.
The AutoMATES project includes three additional modules that all provide input to the SMS pipeline.
These modules are the Program Analysis (PA) pipeline, the Text Reading (TR) module, and the Equation Reading (ER) module.
The inputs provided by these modules will assist the SMS pipeline in extracting abstract representations of source code from the original source code languages, as well as grounding the variables found in that source code to real-world concepts.
Any inputs to the SMS pipeline from these modules will be documented in the sections of the thesis where they are used.

The remainder of this thesis is organized to present the components of the pipeline that will solve the problem presented in the problem scope section.
During the course of this thesis I will use two models from the Decision Support System for Agrotechnology Transfer (DSSAT)\footnote{\url{https://dssat.net/}} software system \citep{DSSAT}.
% model collection to demonstrate the capabilities of the SMS pipeline.
The models used in this study will be targeting the natural phenomena of Potential Evapo-Transpiration (PET).
The specific models I will be comparing are the Priestly-Taylor model of Potential Evapo-Transpiration (PETPT) and the ASCE model of Potential Evapo-Transpiration (PETASCE).
Chapter~\ref{chapter:extraction} will introduce the algorithms used to extract models from source code and transform them into a form that is both executable and comparable across competing models.
Chapter~\ref{chapter:extraction} will also introduce the analysis methods used by the SMS pipeline to derive information about the models under comparison.
Chapter~\ref{chapter:analysis} will document how the derived information from the analysis phase can be used to perform automated model selection, as well as how the information will be presented to users of the SMS pipeline to allow them to make their own final model selection decisions.
This thesis will then conclude with Chapter~\ref{sec:conclusions} that will be a discussion of the results and implications of the pipeline and will introduce possible extensions for continuing this research.
